{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spring_22_NNDL_Lab_10_Solution.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LAB 10 : Restricted Boltzmann Machine\n",
        "\n",
        "Name : \n",
        "\n",
        "Roll Number :\n",
        "\n",
        "References : \n",
        "\n",
        "1. MNIST Dataset : http://yann.lecun.com/exdb/mnist/\n",
        "2. Movie Lens Dataset : https://grouplens.org/datasets/movielens/\n",
        "3. https://towardsdatascience.com/restricted-boltzmann-machine-how-to-create-a-recommendation-system-for-movie-review-45599a406deb\n",
        "4. https://towardsdatascience.com/restricted-boltzmann-machine-as-a-recommendation-system-for-movie-review-part-2-9a6cab91d85b\n",
        "5. https://github.com/echen/restricted-boltzmann-machines"
      ],
      "metadata": {
        "id": "-RKeacOw3vX9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Problem 1** : MNIST Digit Classification using RBM + Logistic Regression \n",
        "\n",
        "1. Consider MNIST Digit Dataset\n",
        "2. Use the Bernoulli RBM API from Sci-kit learn package and create a pipeline of RBM network and logistic regression to classify the digits"
      ],
      "metadata": {
        "id": "7Vtiy88f4ikk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Write down the Objectives, Hypothesis and Experimental description for the above problem\n"
      ],
      "metadata": {
        "id": "TWbtmaWF6AoP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tHFkTwon6B33"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Programming : \n",
        "  Please write a program to demonstrate the same"
      ],
      "metadata": {
        "id": "BevdKu8E6CQm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8liOW29M3ucS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import convolve\n",
        "from sklearn import linear_model, datasets, metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import BernoulliRBM\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "from sklearn.base import clone"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def nudge_dataset(X, Y):\n",
        "  \"\"\"\n",
        "  This produces a dataset 5 times bigger than the original one,\n",
        "  by moving the 8x8 images in X around by 1px to left, right, down, up\n",
        "  \"\"\"\n",
        "  direction_vectors = [\n",
        "  [[0, 1, 0], [0, 0, 0], [0, 0, 0]],\n",
        "  [[0, 0, 0], [1, 0, 0], [0, 0, 0]],\n",
        "  [[0, 0, 0], [0, 0, 1], [0, 0, 0]],\n",
        "  [[0, 0, 0], [0, 0, 0], [0, 1, 0]],\n",
        "  ]\n",
        "  def shift(x, w):\n",
        "    return convolve(x.reshape((8, 8)), mode=\"constant\", weights=w).ravel()\n",
        "  \n",
        "  X = np.concatenate([X] + [np.apply_along_axis(shift, 1, X, vector) for vector in direction_vectors])\n",
        "  Y = np.concatenate([Y for _ in range(5)], axis=0)\n",
        "  return X, Y\n"
      ],
      "metadata": {
        "id": "sck02ZxQiWe-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = datasets.load_digits(return_X_y=True)\n",
        "X = np.asarray(X, \"float32\")\n",
        "X, Y = nudge_dataset(X, y)\n",
        "X = minmax_scale(X, feature_range=(0, 1)) # 0-1 scaling\n"
      ],
      "metadata": {
        "id": "-c6KBveLiqFM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "dHprYBbxisyU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logistic = linear_model.LogisticRegression(solver=\"newton-cg\", tol=1)\n",
        "rbm = BernoulliRBM(random_state=0, verbose=True)\n",
        "rbm_features_classifier = Pipeline(steps=[(\"rbm\", rbm), (\"logistic\", logistic)])\n"
      ],
      "metadata": {
        "id": "oW2j1aEvixv2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper-parameters. These were set by cross-validation,\n",
        "# using a GridSearchCV. Here we are not performing cross-validation to\n",
        "# save time.\n",
        "rbm.learning_rate = 0.06\n",
        "rbm.n_iter = 10\n",
        "# More components tend to give better prediction performance, but larger\n",
        "# fitting time\n",
        "rbm.n_components = 100\n",
        "logistic.C = 6000\n",
        "# Training RBM-Logistic Pipeline\n",
        "rbm_features_classifier.fit(X_train, Y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGYDiEMKiyNE",
        "outputId": "1934582f-39a8-4840-e794-8b7620ee1279"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BernoulliRBM] Iteration 1, pseudo-likelihood = -25.57, time = 0.38s\n",
            "[BernoulliRBM] Iteration 2, pseudo-likelihood = -23.68, time = 0.52s\n",
            "[BernoulliRBM] Iteration 3, pseudo-likelihood = -22.88, time = 0.55s\n",
            "[BernoulliRBM] Iteration 4, pseudo-likelihood = -21.91, time = 0.50s\n",
            "[BernoulliRBM] Iteration 5, pseudo-likelihood = -21.79, time = 0.42s\n",
            "[BernoulliRBM] Iteration 6, pseudo-likelihood = -20.96, time = 0.32s\n",
            "[BernoulliRBM] Iteration 7, pseudo-likelihood = -20.80, time = 0.26s\n",
            "[BernoulliRBM] Iteration 8, pseudo-likelihood = -20.63, time = 0.26s\n",
            "[BernoulliRBM] Iteration 9, pseudo-likelihood = -20.38, time = 0.25s\n",
            "[BernoulliRBM] Iteration 10, pseudo-likelihood = -20.19, time = 0.25s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('rbm',\n",
              "                 BernoulliRBM(learning_rate=0.06, n_components=100,\n",
              "                              random_state=0, verbose=True)),\n",
              "                ('logistic',\n",
              "                 LogisticRegression(C=6000, solver='newton-cg', tol=1))])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_pixel_classifier = clone(logistic)\n",
        "raw_pixel_classifier.C = 100.0\n",
        "raw_pixel_classifier.fit(X_train, Y_train)\n",
        "\n",
        "Y_pred = rbm_features_classifier.predict(X_test)\n",
        "print(\n",
        " \"Logistic regression using RBM features:\\n%s\\n\"\n",
        " % (metrics.classification_report(Y_test, Y_pred))\n",
        ")\n",
        "print(\n",
        " \"Confusion Matrix:\\n%s\\n\"\n",
        " % (metrics.confusion_matrix(Y_test, Y_pred))\n",
        ")\n",
        "Y_pred = raw_pixel_classifier.predict(X_test)\n",
        "print(\n",
        " \"Logistic regression using raw pixel features:\\n%s\\n\"\n",
        " % (metrics.classification_report(Y_test, Y_pred))\n",
        ")\n",
        "print(\n",
        " \"Confusion Matrix:\\n%s\\n\"\n",
        " % (metrics.confusion_matrix(Y_test, Y_pred))\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6_LE0xNi6eS",
        "outputId": "aa2f308b-07b7-4b24-a6eb-f37bf17e8f45"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression using RBM features:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99       174\n",
            "           1       0.90      0.91      0.91       184\n",
            "           2       0.92      0.95      0.93       166\n",
            "           3       0.95      0.89      0.91       194\n",
            "           4       0.95      0.94      0.94       186\n",
            "           5       0.93      0.92      0.93       181\n",
            "           6       0.97      0.96      0.97       207\n",
            "           7       0.93      0.99      0.96       154\n",
            "           8       0.90      0.88      0.89       182\n",
            "           9       0.88      0.91      0.89       169\n",
            "\n",
            "    accuracy                           0.93      1797\n",
            "   macro avg       0.93      0.93      0.93      1797\n",
            "weighted avg       0.93      0.93      0.93      1797\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[171   0   0   0   2   0   1   0   0   0]\n",
            " [  0 168   1   1   1   1   1   1   4   6]\n",
            " [  0   4 158   1   0   0   0   1   2   0]\n",
            " [  0   0   8 172   0   2   0   2   6   4]\n",
            " [  1   1   0   0 175   1   0   4   0   4]\n",
            " [  0   1   1   2   1 167   3   0   2   4]\n",
            " [  0   4   0   0   1   3 199   0   0   0]\n",
            " [  0   1   0   0   0   0   0 152   0   1]\n",
            " [  0   7   3   2   3   3   1   2 160   1]\n",
            " [  0   1   1   4   2   3   0   1   4 153]]\n",
            "\n",
            "Logistic regression using raw pixel features:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.93      0.91       174\n",
            "           1       0.59      0.56      0.57       184\n",
            "           2       0.75      0.85      0.80       166\n",
            "           3       0.78      0.78      0.78       194\n",
            "           4       0.81      0.84      0.83       186\n",
            "           5       0.76      0.77      0.77       181\n",
            "           6       0.91      0.87      0.89       207\n",
            "           7       0.86      0.88      0.87       154\n",
            "           8       0.67      0.58      0.62       182\n",
            "           9       0.74      0.76      0.75       169\n",
            "\n",
            "    accuracy                           0.78      1797\n",
            "   macro avg       0.78      0.78      0.78      1797\n",
            "weighted avg       0.78      0.78      0.78      1797\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[161   2   1   1   3   2   2   0   2   0]\n",
            " [  3 103  14  15  11   8   3   2  16   9]\n",
            " [  0   9 141   7   2   1   1   0   3   2]\n",
            " [  0   7   8 151   1   6   1   3   9   8]\n",
            " [  1   7   0   0 156   1   6   4   3   8]\n",
            " [  0   5  10   7   4 139   2   3   4   7]\n",
            " [  5   8   2   0   4   4 181   0   3   0]\n",
            " [  3   1   3   0   3   1   0 135   4   4]\n",
            " [  1  27   8   7   4  13   3   6 105   8]\n",
            " [  4   6   1   5   4   7   1   4   8 129]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inferences and Conclusion : State all the key observations and conclusion"
      ],
      "metadata": {
        "id": "aYndZndj6FLy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FAV1M3ff6H0I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Problem 2** : RBM as a Recommendation System for Movie Review on Movie Lens Dataset\n",
        "\n",
        "1. Use the Movie Lens Dataset, Split it into train-test set. Convert the ratings to Binary (The task is to predict if the user likes a movie or not) \n",
        "2. Build a RBM network, train the model and test it on the test set"
      ],
      "metadata": {
        "id": "QwXEYR7_6Xik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Write down the Objectives, Hypothesis and Experimental description for the above problem\n"
      ],
      "metadata": {
        "id": "KtJnEBRj7NTt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XgtGYaf07PQM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Programming : \n",
        "  Please write a program to demonstrate the same"
      ],
      "metadata": {
        "id": "9nR68D3E7Pux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "from torch.autograd import Variable"
      ],
      "metadata": {
        "id": "9FTFTo7Cjg33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "dbomSIWY7Rr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##import dataset\n",
        "movies = pd.read_csv('ml-1m/movies.dat', sep = '::',header = None, engine = 'python',\n",
        "                     encoding = 'latin-1')\n",
        "users =pd.read_csv('ml-1m/users.dat', sep = '::', header = None, engine = 'python',\n",
        "                     encoding = 'latin-1')\n",
        "ratings =pd.read_csv('ml-1m/ratings.dat', sep = '::', header = None, engine = 'python',\n",
        "                     encoding = 'latin-1')"
      ],
      "metadata": {
        "id": "YylXjfC5jIMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##create training and test set data\n",
        "training_set = pd.read_csv('ml-100k/u1.base', delimiter = '\\t', header = None)\n",
        "##convert it to array\n",
        "training_set = np.array(training_set, dtype = 'int')\n",
        "\n",
        "test_set = pd.read_csv('ml-100k/u1.test', delimiter = '\\t', header = None)\n",
        "##convert it to array\n",
        "test_set = np.array(test_set, dtype = 'int')"
      ],
      "metadata": {
        "id": "qYlnVRo7jOOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#take max users id in train and test data\n",
        "nb_users = int(max(max(training_set[:, 0]), max(test_set[:, 0])))\n",
        "nb_movies =  int(max(max(training_set[:, 1]), max(test_set[:, 1])))\n"
      ],
      "metadata": {
        "id": "r5ifnXANjoZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert(data):\n",
        "    new_data = []\n",
        "    for id_users in range(1, nb_users + 1):\n",
        "        id_movies = data[:,1][data[:,0] == id_users]\n",
        "        id_ratings = data[:,2][data[:,0] == id_users]\n",
        "        ratings = np.zeros(nb_movies)\n",
        "        ratings[id_movies â€” 1] = id_ratings\n",
        "        new_data.append(list(ratings))\n",
        "    return new_data"
      ],
      "metadata": {
        "id": "jFBPJnhijRKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_set = convert(training_set)\n",
        "test_set = convert(test_set)"
      ],
      "metadata": {
        "id": "Z1BRO3E6jSmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_set = torch.FloatTensor(training_set)\n",
        "test_set = torch.FloatTensor(test_set)"
      ],
      "metadata": {
        "id": "E2Vpkiw6jUCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_set[training_set == 0] = -1\n",
        "training_set[training_set == 1] = 0\n",
        "training_set[training_set == 2] = 0\n",
        "training_set[training_set >= 3] = 1\n",
        "test_set[test_set == 0] = -1\n",
        "test_set[test_set == 1] = 0\n",
        "test_set[test_set == 2] = 0\n",
        "test_set[test_set >= 3] = 1"
      ],
      "metadata": {
        "id": "rU0rZhtIjVov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RBM():\n",
        "    def __init__(self, nv, nh):\n",
        "        ##initialize all weights \n",
        "        ##a tensor with size of nh, nv in normal dis mean 0 var 1\n",
        "        self.W = torch.randn(nh, nv)\n",
        "        #bias for hidden nodes\n",
        "        #1st dimension is batch, 2nd is num of hidden nodes\n",
        "        self.a = torch.randn(1, nh)\n",
        "        #bias for visible nodes\n",
        "        self.b = torch.randn(1, nv)\n",
        "    #activate the hidden nodes by sampling all hiddens node, given values of visible nodes \n",
        "    def sample_h(self, x):\n",
        "        #x is values of visible nodes\n",
        "        #probablity of hiddens h to be activated, given values of visible  nodes v\n",
        "        wx = torch.mm(x, self.W.t())\n",
        "        #use sigmoid fuc to activate visible node\n",
        "        ## a is bias for hidden nodes\n",
        "        activation = wx + self.a.expand_as(wx)\n",
        "        ##ith of the vector is the probability of ith hidden nodes to be activated, \n",
        "        ##given visible values\n",
        "        p_h_given_v =torch.sigmoid(activation)\n",
        "        #samples of all hiddens nodes\n",
        "        return p_h_given_v, torch.bernoulli(p_h_given_v)\n",
        "    def sample_v(self, y):\n",
        "        #y is hidden nodes\n",
        "        #probablity of visible h to be activated, given hidden  nodes v\n",
        "        wy = torch.mm(y, self.W)\n",
        "        #use sigmoid fuc to activate hiddens nodes\n",
        "        activation = wy + self.b.expand_as(wy)\n",
        "        ##ith of the vector is the probability of ith visible nodes to be activated, \n",
        "        ##given hidden values\n",
        "        p_v_given_h =torch.sigmoid(activation)\n",
        "        #samples of all hiddens nodes\n",
        "        return p_v_given_h, torch.bernoulli(p_v_given_h)\n",
        "        \n",
        "    #visible nodes after kth interation\n",
        "    #probablity of hidden nodes after kth iteration\n",
        "    def train(self, v0, vk, ph0, phk):\n",
        "#         self.W += torch.mm(v0.t(), ph0) - torch.mm(vk.t(), phk)\n",
        "        self.W += (torch.mm(v0.t(), ph0) - torch.mm(vk.t(), phk)).t()\n",
        "#         self.W += torch.mm(v0.t(), ph0) - torch.mm(vk.t(), phk)\n",
        "        #add zero to keep b as a tensor of 2 dimension\n",
        "        self.b += torch.sum((v0 - vk), 0)\n",
        "        self.a += torch.sum((ph0 - phk), 0)"
      ],
      "metadata": {
        "id": "fmHpa-NTjyKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#number of movies\n",
        "nv = len(training_set[0])\n",
        "#number of hidden nodes or num of features\n",
        "nh = 100\n",
        "batch_size = 100\n",
        "rbm = RBM(nv, nh)"
      ],
      "metadata": {
        "id": "GvBoXCoWjzFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_epoch = 10\n",
        "for epoch in range(1, nb_epoch+1):\n",
        "    ##loss function\n",
        "    train_loss = 0\n",
        "    #normalize the loss, define a counter\n",
        "    s = 0.\n",
        "    #implement a batch learning, \n",
        "    for id_user in range(0, nb_users - batch_size, 100):\n",
        "        #input batch values\n",
        "        vk = training_set[id_user: id_user+batch_size]\n",
        "        #target used for loss mesarue: rating \n",
        "        v0 = training_set[id_user: id_user+batch_size]\n",
        "        ##initilize probablity\n",
        "        #pho: given real rating at begining, probablity of hidden nodes\n",
        "        ph0, _ = rbm.sample_h(v0)\n",
        "        #k step of constrative divergence\n",
        "        for k in range(10):\n",
        "            _, hk = rbm.sample_h(vk)\n",
        "            _, vk = rbm.sample_v(hk)\n",
        "            #training on rating that do exist, rating as -1\n",
        "            vk[v0<0] = v0[v0<0]\n",
        "        phk, _ = rbm.sample_h(vk)\n",
        "        #update weights and bias\n",
        "        rbm.train(v0, vk, ph0, phk)\n",
        "        #update train loss\n",
        "        train_loss += torch.mean(torch.abs(v0[v0>0]-vk[v0>0]))\n",
        "        s += 1\n",
        "    print('epoch: '+str(epoch)+' loss: '+str(train_loss/s))"
      ],
      "metadata": {
        "id": "LxmSrK5Zj4BX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##loss function\n",
        "test_loss = 0\n",
        "#normalize the loss, define a counter\n",
        "s = 0.\n",
        "#implement a batch learning, \n",
        "for id_user in range(0, nb_users):\n",
        "    #use input of train set to activate RBM\n",
        "    v_input = training_set[id_user: id_user+1]\n",
        "    #target used for loss mesarue: rating \n",
        "    v_target = test_set[id_user: id_user+1]\n",
        "    #use only 1 step to make better prediction, though used 10 steps to train\n",
        "    if len(v_target[v_target>=0]):\n",
        "        _, h = rbm.sample_h(v_input)\n",
        "        _, v_input = rbm.sample_v(h)\n",
        "        #update test loss\n",
        "        test_loss += torch.mean(torch.abs(v_target[v_target>0]-v_input[v_target>0]))\n",
        "        s += 1"
      ],
      "metadata": {
        "id": "_UoP1xRqj5iE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inferences and Conclusion : State all the key observations and conclusion"
      ],
      "metadata": {
        "id": "9tU_BVgW7SLS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JcFjkezc7ypq"
      }
    }
  ]
}