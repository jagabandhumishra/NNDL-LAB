{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spring_22_NNDL_Lab_5_Solution.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LAB 5 : Convolutional Neural Networks  \n",
        "\n",
        "Name : \n",
        "\n",
        "Roll Number : \n",
        "\n",
        "Referrence Material : \n",
        "\n",
        "  MNIST Digit Dataset : http://yann.lecun.com/exdb/mnist/\n",
        "\n",
        "  CNN : https://towardsdatascience.com/a-guide-to-convolutional-neural-networks-from-scratch-f1e3bfc3e2de"
      ],
      "metadata": {
        "id": "7ztwthmd_ENq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Problem 1** : Demonstrate the working principle of convolution neural network (CNN)\n",
        "\n",
        "Observation to be demonstrated:\n",
        "1. Use suitable data to demonstrate the same (MNIST digit data can be used).\n",
        "2. Demonstrate the translation invariant property of pooling layer (can choose two\n",
        "image, from which one is translated and show the output of pooling layer is near\n",
        "to identical for both the images ).\n",
        "3. Demonstrate how backpropagation works with max pooling layer.\n",
        "\n",
        "\n",
        "**Note: Take only two class and a small network, and write you own code to demonstrate the same.**"
      ],
      "metadata": {
        "id": "kq98Srzb_7c6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Write down the Objectives, Hypothesis and Experimental description for the above problem\n"
      ],
      "metadata": {
        "id": "uv6cZthvAJCv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "z2EQWT-fAQ3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Programming : \n",
        "  Please write a program to demonstrate the same"
      ],
      "metadata": {
        "id": "o0ot-6hOAMIr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Y7sAQrAC-GNB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "'''\n",
        "Note: In this implementation, we assume the input is a 2d numpy array for simplicity, because that's\n",
        "how our MNIST images are stored. This works for us because we use it as the first layer in our\n",
        "network, but most CNNs have many more Conv layers. If we were building a bigger network that needed\n",
        "to use Conv multiple times, we'd have to make the input be a 3d numpy array.\n",
        "'''\n",
        "\n",
        "class Conv:\n",
        "  # A Convolution layer using 3x3 filters.\n",
        "\n",
        "  def __init__(self, num_filters):\n",
        "    self.num_filters = num_filters\n",
        "\n",
        "    # filters is a 3d array with dimensions (num_filters, 3, 3)\n",
        "    # We divide by 10 to reduce the variance of our initial values\n",
        "    self.filters = np.random.randn(num_filters, 3, 3) / 10\n",
        "\n",
        "  def iterate_regions(self, image):\n",
        "    '''\n",
        "    Generates all possible 3x3 image regions using valid padding.\n",
        "    - image is a 2d numpy array.\n",
        "    '''\n",
        "    h, w = image.shape\n",
        "\n",
        "    for i in range(h - 2):\n",
        "      for j in range(w - 2):\n",
        "        im_region = image[i:(i + 3), j:(j + 3)]\n",
        "        yield im_region, i, j\n",
        "\n",
        "  def forward(self, input):\n",
        "    '''\n",
        "    Performs a forward pass of the conv layer using the given input.\n",
        "    Returns a 3d numpy array with dimensions (h, w, num_filters).\n",
        "    - input is a 2d numpy array\n",
        "    '''\n",
        "    self.last_input = input\n",
        "\n",
        "    h, w = input.shape\n",
        "    output = np.zeros((h - 2, w - 2, self.num_filters))\n",
        "\n",
        "    for im_region, i, j in self.iterate_regions(input):\n",
        "      output[i, j] = np.sum(im_region * self.filters, axis=(1, 2))\n",
        "\n",
        "    return output\n",
        "\n",
        "  def backprop(self, d_L_d_out, learn_rate):\n",
        "    '''\n",
        "    Performs a backward pass of the conv layer.\n",
        "    - d_L_d_out is the loss gradient for this layer's outputs.\n",
        "    - learn_rate is a float.\n",
        "    '''\n",
        "    d_L_d_filters = np.zeros(self.filters.shape)\n",
        "\n",
        "    for im_region, i, j in self.iterate_regions(self.last_input):\n",
        "      for f in range(self.num_filters):\n",
        "        d_L_d_filters[f] += d_L_d_out[i, j, f] * im_region\n",
        "\n",
        "    # Update filters\n",
        "    self.filters -= learn_rate * d_L_d_filters\n",
        "\n",
        "    # We aren't returning anything here since we use Conv as the first layer in our CNN.\n",
        "    # Otherwise, we'd need to return the loss gradient for this layer's inputs, just like every\n",
        "    # other layer in our CNN.\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fk5BvFaXMpuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaxPool:\n",
        "  # A Max Pooling layer using a pool size of 2.\n",
        "\n",
        "  def iterate_regions(self, image):\n",
        "    '''\n",
        "    Generates non-overlapping 2x2 image regions to pool over.\n",
        "    - image is a 2d numpy array\n",
        "    '''\n",
        "    h, w, _ = image.shape\n",
        "    new_h = h // 2\n",
        "    new_w = w // 2\n",
        "\n",
        "    for i in range(new_h):\n",
        "      for j in range(new_w):\n",
        "        im_region = image[(i * 2):(i * 2 + 2), (j * 2):(j * 2 + 2)]\n",
        "        yield im_region, i, j\n",
        "\n",
        "  def forward(self, input):\n",
        "    '''\n",
        "    Performs a forward pass of the maxpool layer using the given input.\n",
        "    Returns a 3d numpy array with dimensions (h / 2, w / 2, num_filters).\n",
        "    - input is a 3d numpy array with dimensions (h, w, num_filters)\n",
        "    '''\n",
        "    self.last_input = input\n",
        "\n",
        "    h, w, num_filters = input.shape\n",
        "    output = np.zeros((h // 2, w // 2, num_filters))\n",
        "\n",
        "    for im_region, i, j in self.iterate_regions(input):\n",
        "      output[i, j] = np.amax(im_region, axis=(0, 1))\n",
        "\n",
        "    return output\n",
        "\n",
        "  def backprop(self, d_L_d_out):\n",
        "    '''\n",
        "    Performs a backward pass of the maxpool layer.\n",
        "    Returns the loss gradient for this layer's inputs.\n",
        "    - d_L_d_out is the loss gradient for this layer's outputs.\n",
        "    '''\n",
        "    d_L_d_input = np.zeros(self.last_input.shape)\n",
        "\n",
        "    for im_region, i, j in self.iterate_regions(self.last_input):\n",
        "      h, w, f = im_region.shape\n",
        "      amax = np.amax(im_region, axis=(0, 1))\n",
        "\n",
        "      for i2 in range(h):\n",
        "        for j2 in range(w):\n",
        "          for f2 in range(f):\n",
        "            # If this pixel was the max value, copy the gradient to it.\n",
        "            if im_region[i2, j2, f2] == amax[f2]:\n",
        "              d_L_d_input[i * 2 + i2, j * 2 + j2, f2] = d_L_d_out[i, j, f2]\n",
        "\n",
        "    return d_L_d_input"
      ],
      "metadata": {
        "id": "8pipZMfYMqKa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def leakyReLU(x, alpha=0.001):\n",
        "    return x * alpha if x < 0 else x\n",
        "\n",
        "\n",
        "def leakyReLU_derivative(x, alpha=0.01):\n",
        "    return alpha if x < 0 else 1"
      ],
      "metadata": {
        "id": "-lwpq4x9Mr8X"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class FullyConnected:\n",
        "    def __init__(self, input_len, nodes):\n",
        "      # We divide by input_len to reduce the variance of our initial values\n",
        "      self.weights = np.random.randn(input_len, nodes) / input_len\n",
        "      self.biases = np.zeros(nodes)\n",
        "      self.leakyReLU = np.vectorize(leakyReLU)\n",
        "      self.leakyReLU_derivative = np.vectorize(leakyReLU_derivative)\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.last_input_shape = input.shape         # keep track of last input shape before flattening\n",
        "                                                    # for later backward propagation\n",
        "\n",
        "        input = input.flatten()                                 # flatten input\n",
        "\n",
        "        output = np.dot(input, self.weights) + self.biases      # forward propagate\n",
        "\n",
        "        self.leakyReLU(output)\n",
        "\n",
        "        self.last_input = input                    \n",
        "\n",
        "        return output\n",
        "\n",
        "    def backward(self, din, learning_rate=0.005):\n",
        "        self.leakyReLU_derivative(din)\n",
        "\n",
        "        self.last_input = np.expand_dims(self.last_input, axis=1)\n",
        "        din = np.expand_dims(din, axis=1)\n",
        "\n",
        "        dw = np.dot(self.last_input, np.transpose(din))           # loss gradient of final dense layer weights\n",
        "        db = np.sum(din, axis=1).reshape(self.biases.shape)       # loss gradient of final dense layer biases\n",
        "\n",
        "        self.weights -= learning_rate * dw                        # update weights and biases\n",
        "        self.biases -= learning_rate * db\n",
        "\n",
        "        dout = np.dot(self.weights, din)\n",
        "        return dout.reshape(self.last_input_shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "y4eXt2GOMtNo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Softmax:\n",
        "  # A standard fully-connected layer with softmax activation.\n",
        "\n",
        "  def __init__(self, input_len, nodes):\n",
        "    # We divide by input_len to reduce the variance of our initial values\n",
        "    self.weights = np.random.randn(input_len, nodes) / input_len\n",
        "    self.biases = np.zeros(nodes)\n",
        "\n",
        "  def forward(self, input):\n",
        "    '''\n",
        "    Performs a forward pass of the softmax layer using the given input.\n",
        "    Returns a 1d numpy array containing the respective probability values.\n",
        "    - input can be any array with any dimensions.\n",
        "    '''\n",
        "    self.last_input_shape = input.shape\n",
        "\n",
        "    input = input.flatten()\n",
        "    self.last_input = input\n",
        "\n",
        "    input_len, nodes = self.weights.shape\n",
        "\n",
        "    totals = np.dot(input, self.weights) + self.biases\n",
        "    self.last_totals = totals\n",
        "\n",
        "    exp = np.exp(totals)\n",
        "    return exp / np.sum(exp, axis=0)\n",
        "\n",
        "  def backprop(self, d_L_d_out, learn_rate):\n",
        "    '''\n",
        "    Performs a backward pass of the softmax layer.\n",
        "    Returns the loss gradient for this layer's inputs.\n",
        "    - d_L_d_out is the loss gradient for this layer's outputs.\n",
        "    - learn_rate is a float.\n",
        "    '''\n",
        "    # We know only 1 element of d_L_d_out will be nonzero\n",
        "    for i, gradient in enumerate(d_L_d_out):\n",
        "      if gradient == 0:\n",
        "        continue\n",
        "\n",
        "      # e^totals\n",
        "      t_exp = np.exp(self.last_totals)\n",
        "\n",
        "      # Sum of all e^totals\n",
        "      S = np.sum(t_exp)\n",
        "\n",
        "      # Gradients of out[i] against totals\n",
        "      d_out_d_t = -t_exp[i] * t_exp / (S ** 2)\n",
        "      d_out_d_t[i] = t_exp[i] * (S - t_exp[i]) / (S ** 2)\n",
        "\n",
        "      # Gradients of totals against weights/biases/input\n",
        "      d_t_d_w = self.last_input\n",
        "      d_t_d_b = 1\n",
        "      d_t_d_inputs = self.weights\n",
        "\n",
        "      # Gradients of loss against totals\n",
        "      d_L_d_t = gradient * d_out_d_t\n",
        "\n",
        "      # Gradients of loss against weights/biases/input\n",
        "      d_L_d_w = d_t_d_w[np.newaxis].T @ d_L_d_t[np.newaxis]\n",
        "      d_L_d_b = d_L_d_t * d_t_d_b\n",
        "      d_L_d_inputs = d_t_d_inputs @ d_L_d_t\n",
        "\n",
        "      # Update weights / biases\n",
        "      self.weights -= learn_rate * d_L_d_w\n",
        "      self.biases -= learn_rate * d_L_d_b\n",
        "\n",
        "      return d_L_d_inputs.reshape(self.last_input_shape)"
      ],
      "metadata": {
        "id": "8QPqxPQxMuwp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Sigmoid:\n",
        "  # A standard fully-connected layer with sigmoid activation.\n",
        "\n",
        "  def __init__(self, input_len, nodes):\n",
        "    # We divide by input_len to reduce the variance of our initial values\n",
        "    self.weights = np.random.randn(input_len, nodes) / input_len\n",
        "    self.biases = np.zeros(nodes)\n",
        "\n",
        "  def forward(self, input):\n",
        "    '''\n",
        "    Performs a forward pass of the sigmoid layer using the given input.\n",
        "    Returns a 1d numpy array containing the respective probability values.\n",
        "    - input can be any array with any dimensions.\n",
        "    '''\n",
        "    self.last_input_shape = input.shape\n",
        "\n",
        "    input = input.flatten()\n",
        "    self.last_input = input\n",
        "\n",
        "    input_len, nodes = self.weights.shape\n",
        "\n",
        "    totals = np.dot(input, self.weights) + self.biases\n",
        "    self.last_totals = totals\n",
        "    # print('totals ', totals)\n",
        "    # print('exp ',np.exp(-1*totals))\n",
        "\n",
        "    return 1.0 / (1.0 + np.exp(-1*totals))\n",
        "\n",
        "  def backprop(self, d_L_d_out, learn_rate):\n",
        "    '''\n",
        "    Performs a backward pass of the softmax layer.\n",
        "    Returns the loss gradient for this layer's inputs.\n",
        "    - d_L_d_out is the loss gradient for this layer's outputs.\n",
        "    - learn_rate is a float.\n",
        "    '''\n",
        "    # We know only 1 element of d_L_d_out will be nonzero\n",
        "    for i, gradient in enumerate(d_L_d_out):\n",
        "      if gradient == 0:\n",
        "        continue\n",
        "\n",
        "      # print('last totals ', self.last_totals)\n",
        "      out = 1.0 / (1.0 + np.exp(-1*self.last_totals))\n",
        "      d_out_d_t = (1.0 - out) * out\n",
        "      # print('diff ', d_out_d_t)\n",
        "      # print('gradient ', gradient)\n",
        "\n",
        "      # Gradients of totals against weights/biases/input\n",
        "      d_t_d_w = self.last_input\n",
        "      d_t_d_b = 1\n",
        "      d_t_d_inputs = self.weights\n",
        "\n",
        "      # Gradients of loss against totals\n",
        "      d_L_d_t = gradient * d_out_d_t\n",
        "\n",
        "      # Gradients of loss against weights/biases/input\n",
        "      d_L_d_w = d_t_d_w[np.newaxis].T @ d_L_d_t[np.newaxis]\n",
        "      d_L_d_b = d_L_d_t * d_t_d_b\n",
        "      d_L_d_inputs = d_t_d_inputs @ d_L_d_t\n",
        "\n",
        "      # print('dw ', d_L_d_w)\n",
        "      # print('db ', d_L_d_b)\n",
        "      # Update weights / biases\n",
        "      self.weights -= learn_rate * d_L_d_w\n",
        "      self.biases -= learn_rate * d_L_d_b\n",
        "\n",
        "      return d_L_d_inputs.reshape(self.last_input_shape)"
      ],
      "metadata": {
        "id": "at0SHq4oMwTY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "print('X_train: ' + str(train_images.shape))\n",
        "print('Y_train: ' + str(train_labels.shape))\n",
        "print('X_test:  '  + str(test_images.shape))\n",
        "print('Y_test:  '  + str(test_labels.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0Qqoeo3Mx0B",
        "outputId": "06067491-969c-4fdd-e185-3b74b36d6f2e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "X_train: (60000, 28, 28)\n",
            "Y_train: (60000,)\n",
            "X_test:  (10000, 28, 28)\n",
            "Y_test:  (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_filter = np.where((train_labels == 0 ) | (train_labels == 1))\n",
        "test_filter = np.where((test_labels == 0) | (test_labels == 1))"
      ],
      "metadata": {
        "id": "JKQWVKvNM4dt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images, train_labels = train_images[train_filter], train_labels[train_filter]\n",
        "test_images, test_labels = test_images[test_filter], test_labels[test_filter]"
      ],
      "metadata": {
        "id": "5DQY1k45M6Nj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('X_train: ' + str(train_images.shape))\n",
        "print('Y_train: ' + str(train_labels.shape))\n",
        "print('X_test:  '  + str(test_images.shape))\n",
        "print('Y_test:  '  + str(test_labels.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAMKvV4JM7dm",
        "outputId": "70360fec-e7b8-4b4b-8785-5bca14aa3313"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train: (12665, 28, 28)\n",
            "Y_train: (12665,)\n",
            "X_test:  (2115, 28, 28)\n",
            "Y_test:  (2115,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv = Conv(8)                  # 28x28x1 -> 26x26x8\n",
        "pool = MaxPool()                  # 26x26x8 -> 13x13x8\n",
        "softmax = Sigmoid(13 * 13 * 8, 1) # 13x13x8 -> 10\n",
        "\n",
        "def forward(image, label):\n",
        "  '''\n",
        "  Completes a forward pass of the CNN and calculates the accuracy and\n",
        "  cross-entropy loss.\n",
        "  - image is a 2d numpy array\n",
        "  - label is a digit\n",
        "  '''\n",
        "  # We transform the image from [0, 255] to [-0.5, 0.5] to make it easier\n",
        "  # to work with. This is standard practice.\n",
        "  out = conv.forward((image / 255) - 0.5)\n",
        "  out = pool.forward(out)\n",
        "  out = softmax.forward(out)\n",
        "  # print('out ',out)\n",
        "\n",
        "  # Calculate cross-entropy loss and accuracy. np.log() is the natural log.\n",
        "  if (label == 0):\n",
        "    loss = -np.log(1-out)\n",
        "    acc = 1 if (out<0.5) else 0\n",
        "  else:\n",
        "    loss = -np.log(out)\n",
        "    acc = 1 if (out>0.5) else 0\n",
        "  # print('acc ', acc)\n",
        "\n",
        "  return out, loss, acc\n",
        "\n",
        "def train(im, label, lr=.0001):\n",
        "  '''\n",
        "  Completes a full training step on the given image and label.\n",
        "  Returns the cross-entropy loss and accuracy.\n",
        "  - image is a 2d numpy array\n",
        "  - label is a digit\n",
        "  - lr is the learning rate\n",
        "  '''\n",
        "  # Forward\n",
        "  out, loss, acc = forward(im, label)\n",
        "\n",
        "  # Calculate initial gradient\n",
        "  gradient = np.zeros(2)\n",
        "  if (label==0): \n",
        "    gradient[0] = 1 / (1-out)\n",
        "  else:\n",
        "    gradient[1] = -1/out\n",
        "\n",
        "  # Backprop\n",
        "  gradient = softmax.backprop(gradient, lr)\n",
        "  gradient = pool.backprop(gradient)\n",
        "  gradient = conv.backprop(gradient, lr)\n",
        "\n",
        "  return loss, acc\n",
        "\n",
        "print('MNIST CNN initialized!')\n",
        "\n",
        "# Train the CNN for 2 epochs\n",
        "for epoch in range(1):\n",
        "  print('--- Epoch %d ---' % (epoch + 1))\n",
        "\n",
        "  # Shuffle the training data\n",
        "  permutation = np.random.permutation(len(train_images))\n",
        "  train_images = train_images[permutation]\n",
        "  train_labels = train_labels[permutation]\n",
        "\n",
        "  # Train!\n",
        "  loss = 0\n",
        "  num_correct = 0\n",
        "  for i, (im, label) in enumerate(zip(train_images, train_labels)):\n",
        "    if i % 500 == 0 and i>0:\n",
        "      print(\n",
        "        '[Step %d] Past 500 steps: Average Loss %.3f | Accuracy: %.3f' %\n",
        "        (i + 1, loss / 500, num_correct/500)\n",
        "      )\n",
        "      loss = 0\n",
        "      num_correct = 0\n",
        "\n",
        "    l, acc = train(im, label)\n",
        "    loss += l\n",
        "    num_correct += acc\n",
        "\n",
        "# Test the CNN\n",
        "print('\\n--- Testing the CNN ---')\n",
        "loss = 0\n",
        "num_correct = 0\n",
        "for im, label in zip(test_images, test_labels):\n",
        "  _, l, acc = forward(im, label)\n",
        "  loss += l\n",
        "  num_correct += acc\n",
        "\n",
        "num_tests = len(test_images)\n",
        "print('Test Loss:', loss / num_tests)\n",
        "print('Test Accuracy:', num_correct / num_tests)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JlYrYBaM91n",
        "outputId": "6ba171db-ad32-40d4-d82b-659252e440da"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNIST CNN initialized!\n",
            "--- Epoch 1 ---\n",
            "[Step 501] Past 500 steps: Average Loss 0.669 | Accuracy: 0.708\n",
            "[Step 1001] Past 500 steps: Average Loss 0.620 | Accuracy: 0.880\n",
            "[Step 1501] Past 500 steps: Average Loss 0.571 | Accuracy: 0.910\n",
            "[Step 2001] Past 500 steps: Average Loss 0.513 | Accuracy: 0.936\n",
            "[Step 2501] Past 500 steps: Average Loss 0.445 | Accuracy: 0.948\n",
            "[Step 3001] Past 500 steps: Average Loss 0.396 | Accuracy: 0.954\n",
            "[Step 3501] Past 500 steps: Average Loss 0.329 | Accuracy: 0.968\n",
            "[Step 4001] Past 500 steps: Average Loss 0.278 | Accuracy: 0.986\n",
            "[Step 4501] Past 500 steps: Average Loss 0.221 | Accuracy: 0.986\n",
            "[Step 5001] Past 500 steps: Average Loss 0.181 | Accuracy: 0.992\n",
            "[Step 5501] Past 500 steps: Average Loss 0.170 | Accuracy: 0.988\n",
            "[Step 6001] Past 500 steps: Average Loss 0.128 | Accuracy: 0.992\n",
            "[Step 6501] Past 500 steps: Average Loss 0.116 | Accuracy: 0.996\n",
            "[Step 7001] Past 500 steps: Average Loss 0.110 | Accuracy: 0.988\n",
            "[Step 7501] Past 500 steps: Average Loss 0.092 | Accuracy: 0.988\n",
            "[Step 8001] Past 500 steps: Average Loss 0.077 | Accuracy: 0.996\n",
            "[Step 8501] Past 500 steps: Average Loss 0.079 | Accuracy: 0.996\n",
            "[Step 9001] Past 500 steps: Average Loss 0.063 | Accuracy: 0.998\n",
            "[Step 9501] Past 500 steps: Average Loss 0.075 | Accuracy: 0.982\n",
            "[Step 10001] Past 500 steps: Average Loss 0.066 | Accuracy: 0.994\n",
            "[Step 10501] Past 500 steps: Average Loss 0.062 | Accuracy: 0.994\n",
            "[Step 11001] Past 500 steps: Average Loss 0.049 | Accuracy: 1.000\n",
            "[Step 11501] Past 500 steps: Average Loss 0.048 | Accuracy: 1.000\n",
            "[Step 12001] Past 500 steps: Average Loss 0.048 | Accuracy: 0.998\n",
            "[Step 12501] Past 500 steps: Average Loss 0.055 | Accuracy: 0.990\n",
            "\n",
            "--- Testing the CNN ---\n",
            "Test Loss: [0.03897363]\n",
            "Test Accuracy: 0.9966903073286052\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv = Conv(8)                  # 28x28x1 -> 26x26x8\n",
        "pool = MaxPool()                  # 26x26x8 -> 13x13x8\n",
        "softmax = Sigmoid(13 * 13 * 8, 1) # 13x13x8 -> 10\n",
        "\n",
        "def forward(image, label):\n",
        "  '''\n",
        "  Completes a forward pass of the CNN and calculates the accuracy and\n",
        "  cross-entropy loss.\n",
        "  - image is a 2d numpy array\n",
        "  - label is a digit\n",
        "  '''\n",
        "  # We transform the image from [0, 255] to [-0.5, 0.5] to make it easier\n",
        "  # to work with. This is standard practice.\n",
        "  out = conv.forward((image / 255) - 0.5)\n",
        "  # print('conv ',out.shape)\n",
        "  out = pool.forward(out)\n",
        "  # print('maxpooling ',out.shape)\n",
        "  out = softmax.forward(out)\n",
        "  # print('out ',out)\n",
        "\n",
        "  # Calculate cross-entropy loss and accuracy. np.log() is the natural log.\n",
        "  if (label == 0):\n",
        "    loss = -np.log(1-out)\n",
        "    acc = 1 if (out<0.5) else 0\n",
        "  else:\n",
        "    loss = -np.log(out)\n",
        "    acc = 1 if (out>0.5) else 0\n",
        "  # print('acc ', acc)\n",
        "\n",
        "  return out, loss, acc\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M3YvDuo9NAWE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define input data\n",
        "image1 = [[0, 0, 0, 0, 1, 0],\n",
        "\t\t      [0, 0, 0, 0, 0, 0],\n",
        "\t\t      [0, 0, 0, 0, 1, 0],\n",
        "\t\t      [0, 0, 0, 0, 0, 0],\n",
        "\t\t      [0, 0, 0, 0, 1, 0],\n",
        "\t      \t[0, 0, 0, 0, 0, 0]]\n",
        "\n",
        "image1 = np.asarray(image1)\n",
        "\n",
        "image2 = [[0, 0, 0, 0, 0, 0],\n",
        "\t\t      [0, 0, 0, 0, 1, 0],\n",
        "\t\t      [0, 0, 0, 0, 0, 0],\n",
        "\t\t      [0, 0, 0, 0, 1, 0],\n",
        "\t\t      [0, 0, 0, 0, 0, 0],\n",
        "\t      \t[0, 0, 0, 0, 1, 0]]\n",
        "\n",
        "image2 = np.asarray(image2)"
      ],
      "metadata": {
        "id": "Hr7d_YDANCaX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv = Conv(1)                  \n",
        "pool = MaxPool()                  \n",
        "\n",
        "out1 = conv.forward(image1)\n",
        "print('conv ',out1.shape)\n",
        "out_conv = out1.reshape(4,4)\n",
        "print(out_conv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33MfrITQNE0X",
        "outputId": "e796b3db-ceb2-4d5c-aeac-4ee9d07bd410"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv  (4, 4, 1)\n",
            "[[ 0.          0.          0.09706698 -0.03197847]\n",
            " [ 0.          0.          0.04481359 -0.0876778 ]\n",
            " [ 0.          0.          0.09706698 -0.03197847]\n",
            " [ 0.          0.          0.04481359 -0.0876778 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out2 = conv.forward(image2)\n",
        "print('conv ',out2.shape)\n",
        "out_conv = out2.reshape(4,4)\n",
        "print(out_conv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1OFuVTMNGFb",
        "outputId": "8da07158-d081-4e0d-fe50-268da2a49222"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv  (4, 4, 1)\n",
            "[[ 0.          0.          0.04481359 -0.0876778 ]\n",
            " [ 0.          0.          0.09706698 -0.03197847]\n",
            " [ 0.          0.          0.04481359 -0.0876778 ]\n",
            " [ 0.          0.          0.09706698 -0.03197847]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out1 = pool.forward(out1)\n",
        "print('maxpooling ',out1.shape)\n",
        "out_pool = out1.reshape(2,2)\n",
        "print(out_pool)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7K_PVouVNHeE",
        "outputId": "94662637-0bd5-4f60-fe89-ad4739ce221c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "maxpooling  (2, 2, 1)\n",
            "[[0.         0.09706698]\n",
            " [0.         0.09706698]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out2 = pool.forward(out2)\n",
        "print('maxpooling ',out2.shape)\n",
        "out_pool = out2.reshape(2,2)\n",
        "print(out_pool)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "is7r2mvJNJEq",
        "outputId": "1ae13bfd-8a33-4adb-be2d-62354f940dce"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "maxpooling  (2, 2, 1)\n",
            "[[0.         0.09706698]\n",
            " [0.         0.09706698]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inferences and Conclusion : State all the key observations and conclusion"
      ],
      "metadata": {
        "id": "z3o9EbwbANBA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nNegTDIbAQRm"
      }
    }
  ]
}