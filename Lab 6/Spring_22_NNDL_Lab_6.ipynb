{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spring_22_NNDL_Lab_6.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LAB 6 : Recurrent Neural Networks\n",
        "\n",
        "Name : \n",
        "\n",
        "Roll Number : \n",
        "\n",
        "Referrence Material : \n",
        "\n",
        "1. https://github.com/pangolulu/rnn-from-scratch\n",
        "2. https://www.analyticsvidhya.com/blog/2019/01/fundamentals-deep-learning-recurrent-neural-networks-scratch-python/\n"
      ],
      "metadata": {
        "id": "8mqXpEeWhf7X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Problem 1** : Next Token Prediction in a Sequence\n",
        "\n",
        "Observation to be demonstrated:\n",
        "\n",
        "1. Generate the data required\n",
        "2. Represent tokens as indices using dictionaries\n",
        "3. Convert the tokens into vectors using One hot encoding\n",
        "4. Implement Recurrent Neural Network to solve the Next token prediction problem"
      ],
      "metadata": {
        "id": "EtHrn2eXhmdN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Write down the Objectives, Hypothesis and Experimental description for the above problem\n",
        "\n"
      ],
      "metadata": {
        "id": "UQQQrokDhp5W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XnwyeTeVhqpx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Programming : \n"
      ],
      "metadata": {
        "id": "8JoALkrXhrHG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Representing tokens or text**\n",
        "\n",
        "In previous labs we mainly considered data $x \\in \\mathrm{R}^d$, where $d$ is the feature space dimension.\n",
        "With time sequences our data can be represented as $x \\in \\mathrm{R}^{t \\, \\times \\, d}$, where $t$ is the sequence length. \n",
        "This emphasises sequence dependence and that the samples along the sequence are not independent and identically distributed (i.i.d.).\n",
        "We will model functions as $\\mathrm{R}^{t \\, \\times \\, d} \\rightarrow \\mathrm{R}^c$, where $c$ is the amount of classes in the output.\n",
        "\n",
        "There are several ways to represent sequences. With text, the challenge is how to represent a word as a feature vector in $d$ dimensions, as we are required to represent text with decimal numbers in order to apply neural networks to it.\n",
        "\n",
        "In this exercise we will use a simple one-hot encoding but for categorical variables that can take on many values (e.g. words in the English language) this may be infeasible. For such scenarios, you can project the encodings into a smaller space by use of embeddings."
      ],
      "metadata": {
        "id": "h3duSEPZlEq_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **One-hot encoding over vocabulary**\n",
        "\n",
        "One way to represent a fixed amount of words is by making a one-hot encoded vector, which consists of 0s in all cells with the exception of a single 1 in a cell used uniquely to identify each word.\n",
        "\n",
        "| vocabulary    | one-hot encoded vector   |\n",
        "| ------------- |--------------------------|\n",
        "| Paris         | $= [1, 0, 0, \\ldots, 0]$ |\n",
        "| Rome          | $= [0, 1, 0, \\ldots, 0]$ |\n",
        "| Copenhagen    | $= [0, 0, 1, \\ldots, 0]$ |\n",
        "\n",
        "Representing a large vocabulary with one-hot encodings often becomes inefficient because of the size of each sparse vector.\n",
        "To overcome this challenge it is common practice to truncate the vocabulary to contain the $k$ most used words and represent the rest with a special symbol, $\\mathtt{UNK}$, to define unknown/unimportant words.\n",
        "This often causes entities such as names to be represented with $\\mathtt{UNK}$ because they are rare.\n",
        "\n",
        "Consider the following text\n",
        "> I love the corny jokes in Spielberg's new movie.\n",
        "\n",
        "where an example result would be similar to\n",
        "> I love the corny jokes in $\\mathtt{UNK}$'s new movie."
      ],
      "metadata": {
        "id": "OdZUdeSQlbaO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating a dataset\n",
        "\n",
        "For this exercise we will create a simple dataset that we can learn from. We generate sequences of the form:\n",
        "\n",
        "`a a a a b b b b EOS`, `a a b b EOS`, `a a a a a b b b b b EOS`\n",
        "\n",
        "where `EOS` is a special character denoting the end of a sequence. The task is to predict the next token $t_n$, i.e. `a`, `b`, `EOS` or the unknown token `UNK` given the sequence of tokens $\\{ t_{1}, t_{2}, \\dots , t_{n-1}\\}$ and we are to process sequences in a sequential manner. As such, the network will need to learn that e.g. 5 `b`s and an `EOS` token will occur following 5 `a`s."
      ],
      "metadata": {
        "id": "PYhxG_0yljV8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e6FzDlsheny",
        "outputId": "fe3e561b-27a1-4c6c-f945-9a2a6d014b51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A single sample from the generated dataset:\n",
            "['a', 'a', 'a', 'b', 'b', 'b', 'EOS']\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def generate_dataset(num_sequences=10):\n",
        "    \"\"\"\n",
        "    Generates a number of sequences as our dataset.\n",
        "    \n",
        "    Input :\n",
        "     `num_sequences`: the number of sequences to be generated.\n",
        "     \n",
        "    Returns a list of sequences.\n",
        "    \"\"\"\n",
        "    samples = []\n",
        "    \n",
        "    ## Write your code here\n",
        "\n",
        "        \n",
        "    return samples\n",
        "\n",
        "\n",
        "sequences = generate_dataset()\n",
        "\n",
        "print('A single sample from the generated dataset:')\n",
        "print(sequences[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Representing tokens as indices\n",
        "\n",
        "To build a one-hot encoding, we need to assign each possible word in our vocabulary an index. We do that by creating two dictionaries: one that allows us to go from a given word to its corresponding index in our vocabulary, and one for the reverse direction. Let's call them `word_to_idx` and `idx_to_word`. The keyword `num_words` specifies the maximum size of our vocabulary. If we try to access a word that does not exist in our vocabulary, it is automatically replaced by the `UNK` token or its corresponding index."
      ],
      "metadata": {
        "id": "ZOssF34fmCJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from collections import defaultdict\n",
        "\n",
        "def sequences_to_dicts(sequences):\n",
        "    \"\"\"\n",
        "    Create word_to_idx and idx_to_word dictionaries for a list of sequences.\n",
        "    \"\"\"\n",
        "\n",
        "    ## Write your code here\n",
        "\n",
        "\n",
        "    return word_to_idx, idx_to_word, num_sequences, vocab_size\n",
        "\n",
        "\n",
        "word_to_idx, idx_to_word, num_sequences, vocab_size = sequences_to_dicts(sequences)\n",
        "\n",
        "print(\"Word to Index Dictionary : \",dict(word_to_idx))\n",
        "print(\"Index to Word Dictionary : \",dict(idx_to_word))\n",
        "print(\"Number of Sequences : \",num_sequences)\n",
        "print(\"Vocab Size : \",vocab_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kd4-PHrSl-oV",
        "outputId": "285f1f99-b3a7-40fc-971c-b265285d62f0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word to Index Dictionary :  {'a': 0, 'b': 1, 'EOS': 2, 'UNK': 3}\n",
            "Index to Word Dictionary :  {0: 'a', 1: 'b', 2: 'EOS', 3: 'UNK'}\n",
            "Number of Sequences :  10\n",
            "Vocab Size :  4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Dataset \n",
        "\n",
        "To build our dataset, we need to create inputs and targets for each sequences and partition sentences it into training and test sets. 80% and 20% is a common distribution, but mind you that this largely depends on the size of the dataset. **Since we are doing next-word predictions, our target sequence is simply the input sequence shifted by one word.**"
      ],
      "metadata": {
        "id": "qa3EMnoem5RE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    \n",
        "def create_datasets(sequences, p_train=0.8, p_test=0.2):\n",
        "    # Define partition sizes\n",
        "    num_train = int(len(sequences)*p_train)\n",
        "    num_test = int(len(sequences)*p_test)\n",
        "\n",
        "    # Split sequences into partitions\n",
        "    sequences_train = sequences[:num_train]\n",
        "    sequences_test = sequences[-num_test:]\n",
        "\n",
        "    def get_inputs_targets_from_sequences(sequences):\n",
        "        # Define empty lists\n",
        "        inputs, targets = [], []\n",
        "        \n",
        "        # Append inputs and targets s.t. both lists contain L-1 words of a sentence of length L\n",
        "        # but targets are shifted right by one so that we can predict the next word\n",
        "\n",
        "        ## Write your code here\n",
        "\n",
        "\n",
        "            \n",
        "        return inputs, targets\n",
        "\n",
        "    # Get inputs and targets for each partition\n",
        "    inputs_train, targets_train = get_inputs_targets_from_sequences(sequences_train)\n",
        "    inputs_test, targets_test = get_inputs_targets_from_sequences(sequences_test)\n",
        "    \n",
        "\n",
        "    return inputs_train,targets_train,inputs_test,targets_test\n",
        "    \n",
        "\n",
        "x_train,y_train,x_test,y_test = create_datasets(sequences)\n",
        "\n",
        "print(\"Input for the first training sample : \",x_train[0])\n",
        "print(\"Target output for the first training sample : \",y_train[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67EBwhExm6RY",
        "outputId": "e6112598-4667-4e6d-babd-831a28f7963c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input for the first training sample :  ['a', 'a', 'a', 'b', 'b', 'b']\n",
            "Target output for the first training sample :  ['a', 'a', 'b', 'b', 'b', 'EOS']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One-hot encodings\n",
        "\n",
        "We now create a simple function that returns the one-hot encoded representation of a given index of a word in our vocabulary. Notice that the shape of the one-hot encoding is equal to the vocabulary (which can be huge!). Additionally, we define a function to automatically one-hot encode a sentence."
      ],
      "metadata": {
        "id": "XT8pAPFVpr1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encode(idx, vocab_size):\n",
        "    \"\"\"\n",
        "    One-hot encodes a single word given its index and the size of the vocabulary.\n",
        "    \n",
        "    Input :\n",
        "     `idx`: the index of the given word\n",
        "     `vocab_size`: the size of the vocabulary\n",
        "    \n",
        "    Returns a 1-D numpy array of length `vocab_size`.\n",
        "    \"\"\"\n",
        "    # Initialize the encoded array\n",
        "    one_hot = np.zeros(vocab_size)\n",
        "    \n",
        "    # Set the appropriate element to one\n",
        "    one_hot[idx] = 1.0\n",
        "\n",
        "    return one_hot\n",
        "\n",
        "\n",
        "def one_hot_encode_sequence(sequence, vocab_size):\n",
        "    \"\"\"\n",
        "    One-hot encodes a sequence of words given a fixed vocabulary size.\n",
        "    \n",
        "    Input :\n",
        "     `sentence`: a list of words to encode\n",
        "     `vocab_size`: the size of the vocabulary\n",
        "     \n",
        "    Returns a 3-D numpy array of shape (num words, vocab size, 1).\n",
        "    \"\"\"\n",
        "    # Encode each word in the sentence\n",
        "    encoding = np.array([one_hot_encode(word_to_idx[word], vocab_size) for word in sequence])\n",
        "\n",
        "    # Reshape encoding s.t. it has shape (num words, vocab size, 1)\n",
        "    encoding = encoding.reshape(encoding.shape[0], encoding.shape[1], 1)\n",
        "    \n",
        "    return encoding\n",
        "\n",
        "\n",
        "test_word = one_hot_encode(word_to_idx['a'], vocab_size)\n",
        "print(f'Our one-hot encoding of \\'a\\' is {test_word}.')\n",
        "print(f'Our one-hot encoding of \\'a\\' has shape {test_word.shape}.')\n",
        "\n",
        "test_sentence = one_hot_encode_sequence(['a', 'b'], vocab_size)\n",
        "print(f'Our one-hot encoding of \\'a b\\' is {test_sentence}.')\n",
        "print(f'Our one-hot encoding of \\'a b\\' has shape {test_sentence.shape}.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "932mDevcoqh0",
        "outputId": "520baf6b-2fe2-4b3d-f0fb-ebbec7d19c5b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our one-hot encoding of 'a' is [1. 0. 0. 0.].\n",
            "Our one-hot encoding of 'a' has shape (4,).\n",
            "Our one-hot encoding of 'a b' is [[[1.]\n",
            "  [0.]\n",
            "  [0.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [1.]\n",
            "  [0.]\n",
            "  [0.]]].\n",
            "Our one-hot encoding of 'a b' has shape (2, 4, 1).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation of RNN : \n",
        "\n",
        "A recurrent neural network (RNN) is a type of neural network that has been succesful in modelling sequential data, e.g. language, speech, protein sequences, etc.\n",
        "\n",
        "A RNN performs its computations in a cyclic manner, where the same computation is applied to every sample of a given sequence.\n",
        "The idea is that the network should be able to use the previous computations as some form of memory and apply this to future computations.\n",
        "An image may best explain how this is to be understood,\n",
        "\n",
        "![rnn-unroll image](https://github.com/DeepLearningDTU/02456-deep-learning-with-PyTorch/blob/master/static_files/rnn-unfold.png?raw=1)\n",
        "\n",
        "\n",
        "where it the network contains the following elements:\n",
        "\n",
        "- $x$ is the input sequence of samples, \n",
        "- $U$ is a weight matrix applied to the given input sample,\n",
        "- $V$ is a weight matrix used for the recurrent computation in order to pass memory along the sequence,\n",
        "- $W$ is a weight matrix used to compute the output of the every timestep (given that every timestep requires an output),\n",
        "- $h$ is the hidden state (the network's memory) for a given time step, and\n",
        "- $o$ is the resulting output.\n",
        "\n",
        "When the network is unrolled as shown, it is easier to refer to a timestep, $t$.\n",
        "We have the following computations through the network:\n",
        "\n",
        "- $h_t = f(U\\,{x_t} + V\\,{h_{t-1}})$, where $f$ usually is an activation function, e.g. $\\mathrm{tanh}$.\n",
        "- $o_t = \\mathrm{softmax}(W\\,{h_t})$\n",
        "\n",
        "\n",
        "**Steps :** \n",
        "\n",
        "  1. Implement Forward Pass, Backward Pass and Optimisation\n",
        "  2. Write the training loop\n",
        "  3. Take care of the exploding gradient problem by clipping the gradients "
      ],
      "metadata": {
        "id": "ubppcPGHqXnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Write your code here"
      ],
      "metadata": {
        "id": "xN4tNqxwo3i-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inferences and Conclusion : State all the key observations and conclusion"
      ],
      "metadata": {
        "id": "tnwKppT4htoy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xIOyPoSYhv4E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Problem 2** : Demonstrate the same for a Sine Wave\n",
        "\n",
        "Objective : Given a sequence of 50 numbers belonging to a sine wave, predict the 51st number in the series."
      ],
      "metadata": {
        "id": "MPLkrFccsMH8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PeDAyizgsQOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Write your code here"
      ],
      "metadata": {
        "id": "G6-O0EIFt6j2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}